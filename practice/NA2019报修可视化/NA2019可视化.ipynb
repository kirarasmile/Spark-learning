{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import col\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/home/hadoop/anaconda3/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDate(inputStr):\n",
    "    newStr = \"\"\n",
    "    if len(inputStr) == 8:\n",
    "        s1 = inputStr[0:4]\n",
    "        s2 = inputStr[5:6]\n",
    "        s3 = inputStr[7]\n",
    "        newStr = s1 + \"-\" + \"0\" + s2 + \"-\" + \"0\" + s3\n",
    "    elif len(inputStr) == 9:\n",
    "        s1 = inputStr[0:4]\n",
    "        if inputStr[6] == '/':\n",
    "            s2 = inputStr[5:6]\n",
    "            s3 = inputStr[7:9]\n",
    "            newStr = s1 + \"-\" + \"0\" + s2 + \"-\" + s3\n",
    "        else:\n",
    "            s2 = inputStr[5:7]\n",
    "            s3 = inputStr[8]\n",
    "            newStr = s1 + \"-\" + s2 + \"-\" + \"0\" + s3\n",
    "    else:\n",
    "        s1 = inputStr[0:4]\n",
    "        s2 = inputStr[5:7]\n",
    "        s3 = inputStr[8:10]\n",
    "        newStr = s1 + \"-\" + s2 + \"-\" + s3\n",
    "    date = datetime.strptime(newStr, \"%Y-%m-%d\")\n",
    "    return date\n",
    "\n",
    "def getDstCoarse(inputStr):\n",
    "    inputStr = inputStr.lstrip()\n",
    "    newdst = \"\"\n",
    "    if inputStr == '海澜之家':\n",
    "        newdst = inputStr\n",
    "    else:\n",
    "        if len(inputStr) == 4:\n",
    "            newdst = inputStr[0]\n",
    "        elif len(inputStr) == 5:\n",
    "            newdst = inputStr[0]\n",
    "        else:\n",
    "            newdst = 'other'  \n",
    "    return newdst\n",
    "def getDstFine(inputStr):\n",
    "    inputStr = inputStr.lstrip()\n",
    "    newdst = \"\"\n",
    "    if inputStr == '海澜之家':\n",
    "        newdst = inputStr\n",
    "    else:\n",
    "        if len(inputStr) == 4:\n",
    "            newdst = inputStr[0]\n",
    "        elif len(inputStr) == 5:\n",
    "            newdst = inputStr[0] + inputStr[1]\n",
    "        else:\n",
    "            newdst = 'other'    \n",
    "    return newdst\n",
    "\n",
    "        \n",
    "spark = SparkSession.builder.config(conf = SparkConf()).getOrCreate()\n",
    "fields = [StructField(\"date\", DateType(), False),\\\n",
    "          StructField(\"date_order\", IntegerType(), False),\\\n",
    "          StructField(\"dst\", StringType(), True),\\\n",
    "          StructField(\"dst_type\", IntegerType(), False),\\\n",
    "          StructField(\"status\", IntegerType(), False),\n",
    "         ]\n",
    "schema = StructType(fields)\n",
    "rdd0 = spark.sparkContext.textFile(\"/user/hadoop/2019.txt\")\n",
    "rdd1 = rdd0.map(lambda x : x.split(\"\\t\")).map(lambda p : Row(toDate(p[0]), int(p[1]), getDstCoarse(p[2]), int(p[3]), int(p[4])))\n",
    "\n",
    "shemaNAInfo = spark.createDataFrame(rdd1, schema)\n",
    "shemaNAInfo.createOrReplaceTempView(\"naInfo\")\n",
    "\n",
    "# 1.计算每日的报修单数\n",
    "df = shemaNAInfo.filter(col(\"status\") == 5).groupBy(\"date\").agg(func.count(\"date_order\")).sort(shemaNAInfo[\"date\"].asc())\n",
    "\n",
    "# 列重命名\n",
    "df1 = df.withColumnRenamed(\"count(date_order)\", \"date_order\")\n",
    "df1.repartition(1).write.json(\"NA1.json\") # 写入hdfs\n",
    "\n",
    "# 注册为临时表\n",
    "df1.createOrReplaceTempView(\"ustotal\")\n",
    "\n",
    "# 2.计算每栋楼报修数\n",
    "df2 = spark.sql(\"select dst, count(dst) as cdst from naInfo where status = 5 group by dst\")\n",
    "df2.sort(df2[\"dst\"].asc()).repartition(1).write.json(\"NA2.json\")  # 写入hdfs\n",
    "\n",
    "# 3.男女报修数\n",
    "\n",
    "df3 = spark.sql(\"select dst_type, count(dst_type) as sexcount from naInfo where status = 5 group by dst_type\")\n",
    "df3.sort(df3[\"dst_type\"].asc()).repartition(1).write.json(\"NA3.json\") # 写入hdfs\n",
    "\n",
    "# 4.每月报修数\n",
    "df4 = spark.sql(\"select MONTH(date) as month, count(MONTH(date)) as count from naInfo where status = 5 group by month\")\n",
    "df4.sort(df4[\"month\"].asc()).repartition(1).write.json(\"NA4.json\") # 写入hdfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
